{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4f2260-805d-4863-9c88-0afc1b40238a",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## normalize texts\n",
    "\n",
    "- use CAD\n",
    "\n",
    "## split corpus into sentences\n",
    "\n",
    "- use pysbd\n",
    "- safe these as .csv - > IF, IIIIFFFFF, possible, otherwise use .jsonl\n",
    "\n",
    "## create pattern for Prodigy\n",
    "\n",
    "- use se-list from Berenike and Giulia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e88a0d1-409b-44f6-b301-de1a9a044d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111843d2-1acc-4bf4-944a-083e122ed2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"metadata.csv\", encoding=\"utf8\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e345d84-de43-4a9b-b406-31b5e64bd2dc",
   "metadata": {},
   "source": [
    "### normalize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d14c68-7f51-4473-b117-16bca5f1b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Union\n",
    "import warnings\n",
    "import requests\n",
    "from more_itertools import divide\n",
    "\n",
    "CAB_URL = 'https://www.deutschestextarchiv.de/public/cab/query?clean=1&qname=q&a=default1.1800-1900&fmt=raw&file=C%3A%5Cfakepath%5Ctest.txt'\n",
    "CAB_HEADERS = headers = {'Content-Type': 'text/plain'}\n",
    "\n",
    "def cab(text: str, delay: Union[float, None] = None) -> Union[str, None]:\n",
    "\n",
    "    \"\"\"\n",
    "    Queries the CAB-Webservice provided by Deutsches Textarchiv for orthographic normalisation.\n",
    "    If you use this function repeatedly (e.g. in a loop), please use with delay parameter to avoid overloading the server.\n",
    "    If a texts exceeds the size of one megabyte, it is split into smaller parts\n",
    "    and then sent to the service iteratively.\n",
    "    \"\"\"\n",
    "\n",
    "    if delay is not None:\n",
    "        time.sleep(delay)\n",
    "\n",
    "    n_megabytes = len(text.encode('utf-8')) // 1000000\n",
    "\n",
    "    if n_megabytes >= 1:\n",
    "        parts = list(divide(n=n_megabytes+1, iterable=text.split(' ')))\n",
    "        parts = [' '.join(part) for part in parts]\n",
    "    else:\n",
    "        parts = [text]\n",
    "\n",
    "    normed_parts = []\n",
    "    for part in parts:\n",
    "        r = requests.post(url=CAB_URL, headers=CAB_HEADERS,\n",
    "                          data=part.encode('UTF-8'))\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            warnings.warn(\n",
    "                f'Request returned with error code {r.status_code}\\nError Message\\n{r.body}')\n",
    "            return None\n",
    "\n",
    "        normed_parts.append(r.text.strip())\n",
    "\n",
    "    return ' '.join(normed_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a198d7-fa43-4a04-9c96-824ab7708338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:06<00:00, 66.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# To-Do: \n",
    "# - Mouse-Wiggler\n",
    "# look here: https://stackoverflow.com/questions/1181464/controlling-mouse-with-python\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "corpus_files = list(df[\"filename\"])\n",
    "cleaned_files = os.listdir(\"src/corpus_cleaned/\")\n",
    "\n",
    "files_to_CAB = list(set(corpus_files).difference(cleaned_files))\n",
    "\n",
    "for file in tqdm(broken_files):\n",
    "    with open(\"src/corpus/\" + file, \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    try:\n",
    "        cleaned_text = cab(text, 3)\n",
    "        \n",
    "        with open(\"src/corpus_cleaned/\" + file, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(cleaned_text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    with open(\"log.txt\", \"a\", encoding = \"utf8\") as f:\n",
    "        f.write(file + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fcace1a-4456-44e8-b09a-ba886617cfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_files=[]\n",
    "\n",
    "for file in cleaned_files:\n",
    "    with open(\"src/corpus_cleaned/\" + file, \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    if len(text.split())<5:\n",
    "        broken_files.append(file)\n",
    "    \n",
    "len(broken_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b9c844-d3dd-4c36-8632-39a853992916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6f297-149a-4ea8-b7ed-633031d5f5a8",
   "metadata": {},
   "source": [
    "### split into senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d3e289-b9cc-4da8-8c48-f5a560d1e9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "179c6a6c-fbc9-4718-969e-6f79f121106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "header=\"sentence\\n\"\n",
    "\n",
    "with open(\"annotation_data.tsv\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9175bad8-be40-472a-a6c4-f83b5e5499e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 170/1405 [14:53:19<108:09:43, 315.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc/corpus_cleaned/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mseg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation_data.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(sentence \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\data_preprocessing\\lib\\site-packages\\pysbd\\segmenter.py:87\u001b[0m, in \u001b[0;36mSegmenter.segment\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     85\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleaner(text)\u001b[38;5;241m.\u001b[39mclean()\n\u001b[1;32m---> 87\u001b[0m postprocessed_sents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m sentence_w_char_spans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_with_char_spans(postprocessed_sents)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_span:\n",
      "File \u001b[1;32m~\\.conda\\envs\\data_preprocessing\\lib\\site-packages\\pysbd\\processor.py:34\u001b[0m, in \u001b[0;36mProcessor.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m li \u001b[38;5;241m=\u001b[39m ListItemReplacer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m li\u001b[38;5;241m.\u001b[39madd_line_break()\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_abbreviations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_numbers()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_continuous_punctuation()\n",
      "File \u001b[1;32m~\\.conda\\envs\\data_preprocessing\\lib\\site-packages\\pysbd\\processor.py:180\u001b[0m, in \u001b[0;36mProcessor.replace_abbreviations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_abbreviations\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabbreviations_replacer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\data_preprocessing\\lib\\site-packages\\pysbd\\lang\\deutsch.py:66\u001b[0m, in \u001b[0;36mDeutsch.AbbreviationReplacer.replace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m SingleLowerCaseLetterAtStartOfLineRule \u001b[38;5;241m=\u001b[39m Rule(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=^[a-z])\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.(?=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m∯\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m Text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mPossessiveAbbreviationRule,\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mSingleLetterAbbreviationRules\u001b[38;5;241m.\u001b[39mAll,\n\u001b[0;32m     63\u001b[0m         SingleLowerCaseLetterRule,\n\u001b[0;32m     64\u001b[0m         SingleLowerCaseLetterAtStartOfLineRule)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_for_abbreviations_in_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_multi_period_abbreviations()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m Text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mAmPmRules\u001b[38;5;241m.\u001b[39mAll)\n",
      "File \u001b[1;32m~\\.conda\\envs\\data_preprocessing\\lib\\site-packages\\pysbd\\abbreviation_replacer.py:92\u001b[0m, in \u001b[0;36mAbbreviationReplacer.search_for_abbreviations_in_string\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     90\u001b[0m     char_array \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(next_word_start, text)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind, match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(abbrev_match):\n\u001b[1;32m---> 92\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_for_replacements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_array\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32m~\\.conda\\envs\\data_preprocessing\\lib\\site-packages\\pysbd\\lang\\deutsch.py:73\u001b[0m, in \u001b[0;36mDeutsch.AbbreviationReplacer.scan_for_replacements\u001b[1;34m(self, txt, am, index, character_array)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscan_for_replacements\u001b[39m(\u001b[38;5;28mself\u001b[39m, txt, am, index, character_array):\n\u001b[1;32m---> 73\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(?<=\u001b[39;49m\u001b[38;5;132;43;01m{am}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.(?=\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m∯\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m txt\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pysbd\n",
    "import win32api\n",
    "import time\n",
    "import math\n",
    "import ctypes\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# initalize sentencizer\n",
    "seg = pysbd.Segmenter(language=\"de\", # language is German\n",
    "                      clean=True) # cleans unneccesary punctuation, which reduces filesize (a little). Since we only look at tokens, this is ok\n",
    "\n",
    "for file in tqdm(cleaned_files[503:]):\n",
    "    with open(\"src/corpus_cleaned/\" + file, \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    for sentence in seg.segment(text):\n",
    "        with open(\"annotation_data.tsv\", \"a\", encoding=\"utf8\") as f:\n",
    "            f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddf0fab-3a30-4823-bbdd-164aecedd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519688b8-993f-4df9-b4f6-be4bd8478b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df[\"text\"]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173d9fc-9299-4cd0-9ed3-df5b72aa8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a277704-c302-42f3-bd0d-f3d9cfd059b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"annotation_data.csv\", sep=\"\\t\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abe28ba-c477-4781-9636-b8185f00b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_df = pd.read_excel(\"all_entities_int_EN.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0b1416-6714-4d6f-a816-3545d5d3a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>word</th>\n",
       "      <th>type</th>\n",
       "      <th>Austria</th>\n",
       "      <th>Italy</th>\n",
       "      <th>Germany</th>\n",
       "      <th>France</th>\n",
       "      <th>Switzerland</th>\n",
       "      <th>type_grouped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furniture</td>\n",
       "      <td>Abe</td>\n",
       "      <td>interior</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>furniture</td>\n",
       "      <td>Abort</td>\n",
       "      <td>interior</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>architecture</td>\n",
       "      <td>Abstellkammer</td>\n",
       "      <td>interior</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>architecture</td>\n",
       "      <td>Arbeitszimmer</td>\n",
       "      <td>interior</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>architecture</td>\n",
       "      <td>Attika</td>\n",
       "      <td>interior</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTERIOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category           word      type  Austria  Italy  Germany  France  \\\n",
       "0     furniture            Abe  interior      NaN    NaN      NaN     NaN   \n",
       "1     furniture          Abort  interior      NaN    NaN      1.0     NaN   \n",
       "2  architecture  Abstellkammer  interior      NaN    NaN      NaN     NaN   \n",
       "3  architecture  Arbeitszimmer  interior      NaN    NaN      NaN     NaN   \n",
       "4  architecture         Attika  interior      NaN    NaN      NaN     NaN   \n",
       "\n",
       "   Switzerland type_grouped  \n",
       "0          NaN     INTERIOR  \n",
       "1          NaN     INTERIOR  \n",
       "2          NaN     INTERIOR  \n",
       "3          NaN     INTERIOR  \n",
       "4          NaN     INTERIOR  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2546cd48-faf9-4683-aa4c-8e44cc622265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "village         124812\n",
       "mountain         25561\n",
       "stream_lake      13908\n",
       "city              5487\n",
       "forest            2553\n",
       "valley             532\n",
       "nat_terms          531\n",
       "rural              271\n",
       "urban              258\n",
       "furniture           74\n",
       "architecture        48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7861972b-67c5-4a96-8b76-ad5e893350ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_entities = []\n",
    "\n",
    "for entity in list(se_df[\"word\"].loc[se_df[\"category\"]!=\"furniture\"]):\n",
    "    spatial_entities.append([{\"lower\": str(entity).lower()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5a34a8-d46e-470f-9a36-f57d5c2a0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "\n",
    "for spatial_entity in spatial_entities:\n",
    "    label.append(\"SpatialEntity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a532867-73b4-46f4-a763-3d1fe5d5509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'lower': 'abstellkammer'}],\n",
       " [{'lower': 'arbeitszimmer'}],\n",
       " [{'lower': 'attika'}],\n",
       " [{'lower': 'bad'}],\n",
       " [{'lower': 'badezimmer'}]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_entities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "539c8f0d-b852-49d9-8a0f-c3a3430f6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_df = pd.DataFrame()\n",
    "patterns_df[\"label\"]=label\n",
    "patterns_df[\"pattern\"]=spatial_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65f45035-b179-4ad2-98b6-a3db6d930b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pattern</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'abstellkammer'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'arbeitszimmer'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'attika'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'bad'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'badezimmer'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173972</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'niderbauen-chulm'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173973</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'kronberg'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173974</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'farneren'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173975</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'schwändiblueme'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173976</th>\n",
       "      <td>SpatialEntity</td>\n",
       "      <td>[{'lower': 'walenmattweid'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173977 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                label                          pattern\n",
       "0       SpatialEntity     [{'lower': 'abstellkammer'}]\n",
       "1       SpatialEntity     [{'lower': 'arbeitszimmer'}]\n",
       "2       SpatialEntity            [{'lower': 'attika'}]\n",
       "3       SpatialEntity               [{'lower': 'bad'}]\n",
       "4       SpatialEntity        [{'lower': 'badezimmer'}]\n",
       "...               ...                              ...\n",
       "173972  SpatialEntity  [{'lower': 'niderbauen-chulm'}]\n",
       "173973  SpatialEntity          [{'lower': 'kronberg'}]\n",
       "173974  SpatialEntity          [{'lower': 'farneren'}]\n",
       "173975  SpatialEntity    [{'lower': 'schwändiblueme'}]\n",
       "173976  SpatialEntity     [{'lower': 'walenmattweid'}]\n",
       "\n",
       "[173977 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "174afd2f-f7e5-431e-8a24-3385aa122656",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_df.dropna().to_json(\n",
    "    \"pattern.jsonl\", \n",
    "    orient=\"records\", \n",
    "    lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
